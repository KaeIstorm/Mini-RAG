Mini RAG: A Retrieval-Augmented Generation ApplicationLive Demo URL[Your Vercel Deployment URL Here]Project SummaryThis project is a Retrieval-Augmented Generation (RAG) application that enables users to chat with their custom documents in real time. The backend, built with FastAPI and LangChain, processes user-provided text, embeds it into a vector database, and uses a powerful RAG pipeline to answer questions based on the document's content. The frontend, a modern Next.js application, provides a clean and intuitive interface for document ingestion and real-time querying.Core TechnologiesBackendFastAPI: A high-performance Python web framework for building the API.LangChain: The framework that orchestrates the entire RAG pipeline.Pinecone: A cloud-hosted vector database for storing and retrieving document embeddings.Google Gemini API: Used for generating high-quality text embeddings (models/embedding-001) and for text generation (gemini-2.5-pro).Cohere Rerank: An external API used to improve the accuracy of search results.FrontendNext.js: A React framework for building a fast, scalable, and responsive user interface.Tailwind CSS: A utility-first CSS framework for a modern and clean design.Project ArchitectureThe application is structured into two main parts: an offline indexing process and a real-time query API. This separation of concerns ensures that the application is fast, scalable, and efficient.The workflow is as follows:Indexing (Offline/On-Demand): Raw documents are loaded, split into chunks, and embedded into vectors. These vectors are then stored in a Pinecone index.Retrieval: When a user submits a query, a sophisticated retriever first fetches a broad set of relevant documents from the Pinecone index. A Cohere Reranker then refines these results to ensure the most relevant documents are passed to the LLM.Generation: The Google Gemini Pro model receives the user's question along with the highly-relevant document chunks. It generates a concise, grounded answer with inline citations.Quick StartFollow these steps to get the application running locally.1. Environment SetupClone the repository: git clone [your_repo_url]Create a Python virtual environment in the api folder and install dependencies:cd api
pip install -r requirements.txt
Create a Next.js frontend in the frontend folder and install dependencies:cd ../frontend
npm install
Configure environment variables:Get your API keys for Google Gemini, Pinecone, and Cohere.Create a .env file in the root directory and add your keys:GOOGLE_API_KEY=your_gemini_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=your_pinecone_index_name
COHERE_API_KEY=your_cohere_api_key
2. Initial Document IndexingBefore starting the API, you must populate the vector database with your initial documents.From the project root, run the indexing script:python indexing.py
This script will load docs/file.txt, embed the content, and upsert it to your Pinecone index.3. Run the ApplicationOpen your first terminal in the api folder and start the FastAPI server:uvicorn app:app --reload
Open a second terminal in the frontend folder and start the Next.js development server:npm run dev
The application will be available at http://localhost:3000.Remarks[Provide a short summary of your design choices here. Discuss any trade-offs you made, such as using a simpler chunking strategy, and how you would improve the project if you had more time.]